    "for layer_idx, layer in enumerate(model.language_model.model.layers):\\n",
    "    # print(f\"Processing layer {layer_idx}...\")\\n",
    "    experts_group = layer.feed_forward.experts\\n",
    "\\n",
    "    components_to_prune = []\\n",
    "    if hasattr(experts_group, 'gate_proj') and hasattr(experts_group, 'up_proj') and hasattr(experts_group, 'down_proj'):\\n",
    "        components_to_prune.extend([experts_group.gate_proj, experts_group.up_proj, experts_group.down_proj])\\n",
    "    elif hasattr(experts_group, 'fc1') and hasattr(experts_group, 'fc2'):\\n",
    "        components_to_prune.extend([experts_group.fc1, experts_group.fc2])\\n",
    "    else:\\n",
    "        # print(f\"Warning: Layer {layer_idx} - experts_group (type: {type(experts_group)}) does not have expected proj/fc attributes.\")\\n",
    "        continue\\n",
    "\\n",
    "    for comp_idx, component in enumerate(components_to_prune):\\n",
    "        if not (hasattr(component, 'weight') and hasattr(component, 'scales') and hasattr(component, 'biases')):\\n",
    "            # print(f\"  Skipping component {comp_idx} in layer {layer_idx} (type: {type(component)}), missing one or more of: weight, scales, biases (quantization parameters).\")\\n",
    "            continue\\n",
    "\\n",
    "        # print(f\"  Pruning component {comp_idx} (type: {type(component)}) in layer {layer_idx}.\")\\n",
    "        # print(f\"    Old shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\\n",
    "\\n",
    "        component.weight = component.weight[:1]\\n",
    "        component.scales = component.scales[:1]\\n",
    "        component.biases = component.biases[:1]  # Quantization biases\\n",
    "\\n",
    "        # print(f\"    New shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\\n",
    "\\n",
    "        if hasattr(component, 'bias') and component.bias is not None:\\n",
    "            # print(f\"    Old additive bias shape: {component.bias.shape}\")\\n",
    "            component.bias = component.bias[:1]  # Additive bias\\n",
    "            # print(f\"    New additive bias shape: {component.bias.shape}\")\\n",
    "        # else:\\n",
    "            # print(f\"    Component {comp_idx} in layer {layer_idx} has no additive bias or it is None.\")\\n",
    "\\n",
    "    # The following lines were in the original cell for per-layer param count, kept commented\\n",
    "    # layer_params = sum(v.size for _, v in tree_flatten(layer.parameters()))\\n",
    "    # print(f\\\"Layer {layer_idx} parameters after pruning: {layer_params:,}\\\")\\n",
    "    # print(f\"Finished processing layer {layer_idx}.\")\\n",
    "\\n",
    "num_params_after = sum(v.size for _, v in tree_flatten(model.parameters()))\\n",
    "print(f\"Total parameters after pruning: {num_params_after:,}\")\\n"
   ]
  },
