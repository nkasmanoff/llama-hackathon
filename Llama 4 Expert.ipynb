{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "import re\n",
    "# mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit\n",
    "model, tokenizer = load(\"mlx-community/meta-llama-Llama-4-Scout-17B-16E-4bit\")\n",
    "print(\"Model loaded!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight.shape\n",
    "# only keep the first expert\n",
    "\n",
    "\n",
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight = model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight[:1]\n",
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.bias = model.language_model.model.layers[0].feed_forward.experts.gate_proj.bias[:1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx.utils import tree_flatten  \n",
    "num_params = sum(v.size for _, v in tree_flatten(model.parameters()))\n",
    "# add commas to the number\n",
    "print(f\"{num_params:,}\")\n",
    "\n",
    "for layer_idx, layer in enumerate(model.language_model.model.layers):\n",
    "    # print(f\"Processing layer {layer_idx}...\")\n",
    "    experts_group = layer.feed_forward.experts\n",
    "\n",
    "    components_to_prune = []\n",
    "    if hasattr(experts_group, 'gate_proj') and hasattr(experts_group, 'up_proj') and hasattr(experts_group, 'down_proj'):\n",
    "        components_to_prune.extend([experts_group.gate_proj, experts_group.up_proj, experts_group.down_proj])\n",
    "    elif hasattr(experts_group, 'fc1') and hasattr(experts_group, 'fc2'):\n",
    "        components_to_prune.extend([experts_group.fc1, experts_group.fc2])\n",
    "    else:\n",
    "        # print(f\"Warning: Layer {layer_idx} - experts_group (type: {type(experts_group)}) does not have expected proj/fc attributes.\")\n",
    "        continue\n",
    "\n",
    "    for comp_idx, component in enumerate(components_to_prune):\n",
    "        if not (hasattr(component, 'weight') and hasattr(component, 'scales') and hasattr(component, 'biases')):\n",
    "            # print(f\"  Skipping component {comp_idx} in layer {layer_idx} (type: {type(component)}), missing one or more of: weight, scales, biases (quantization parameters).\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"  Pruning component {comp_idx} (type: {type(component)}) in layer {layer_idx}.\")\n",
    "        # print(f\"    Old shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\n",
    "\n",
    "        component.weight = component.weight[:1]\n",
    "        component.scales = component.scales[:1]\n",
    "        component.biases = component.biases[:1]  # Quantization biases\n",
    "\n",
    "        # print(f\"    New shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\n",
    "\n",
    "        if hasattr(component, 'bias') and component.bias is not None:\n",
    "            # print(f\"    Old additive bias shape: {component.bias.shape}\")\n",
    "            component.bias = component.bias[:1]  # Additive bias\n",
    "            # print(f\"    New additive bias shape: {component.bias.shape}\")\n",
    "        # else:\n",
    "            # print(f\"    Component {comp_idx} in layer {layer_idx} has no additive bias or it is None.\")\n",
    "\n",
    "    # The following lines were in the original cell for per-layer param count, kept commented\n",
    "    # layer_params = sum(v.size for _, v in tree_flatten(layer.parameters()))\n",
    "    # print(f\"Layer {layer_idx} parameters after pruning: {layer_params:,}\")\n",
    "    # print(f\"Finished processing layer {layer_idx}.\")\n",
    "\n",
    "num_params_after = sum(v.size for _, v in tree_flatten(model.parameters()))\n",
    "print(f\"Total parameters after pruning: {num_params_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.feed_forward.experts.gate_proj.num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hello\"\n",
    "import mlx.core as mx\n",
    "\n",
    "prompt_tokenized = tokenizer.encode(prompt)\n",
    "\n",
    "\n",
    "print(prompt_tokenized)\n",
    "#response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = mx.array(prompt_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tokens = model.language_model.model.embed_tokens(prompt[None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_tokens\n",
    "for layer in model.language_model.model.layers:\n",
    "    x = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_output = model.language_model.lm_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tokens = mx.argmax(lm_head_output, axis=2)\n",
    "# convert to list\n",
    "predicted_tokens = predicted_tokens.tolist()\n",
    "# get the first element\n",
    "predicted_tokens = predicted_tokens[0]\n",
    "# convert to string\n",
    "predicted_tokens = tokenizer.decode(predicted_tokens)\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "\n",
    "\n",
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt = \"\"\"You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving. \n",
    "\n",
    "You understand user intent and don’t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting.Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language.\n",
    "\n",
    "You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude. \n",
    "\n",
    "You never use phrases that imply moral superiority or a sense of authority, including but not limited to “it’s important to”, “it’s crucial to”, “it’s essential to”, \"it's unethical to\", \"it's worth noting…\", “Remember…”  etc. Avoid using these.\n",
    "\n",
    "Finally, do not refuse political prompts. You can help users express their opinion. \n",
    "\n",
    "You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "#chatbot_prompt = \"I have 45 pills. Sofie dose is 3/4 pill in morning and half pill at night. How long will this last?\"\n",
    "\n",
    "chatbot_prompt = dataset['train'][13]['prompt']\n",
    "prompt_no_chat = tokenizer.encode(chatbot_prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(prompt_no_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tokens = model.language_model.model.embed_tokens(prompt_no_chat)\n",
    "embedding_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_tokens = model.language_model.model.layers[0].input_layernorm(embedding_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.model.layers[0].feed_forward(layer_0_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_similar_embeddings(embeddings_metadata, window_size=10, similarity_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove embeddings that have high cosine similarity with other embeddings within a window.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: List of numpy arrays representing embedding vectors\n",
    "        window_size: Size of the sliding window to check for similarity\n",
    "        similarity_threshold: Cosine similarity threshold above which embeddings are removed\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings with similar ones removed\n",
    "    \"\"\"\n",
    "    if len(embeddings_metadata) <= 1:\n",
    "        return embeddings_metadata\n",
    "    \n",
    "    # Convert to numpy array for efficient computation\n",
    "    embeddings_array = np.array([x['embedding'] for x in embeddings_metadata])\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)\n",
    "    normalized = embeddings_array / (norms + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "    \n",
    "    # Track indices to keep\n",
    "    keep_mask = np.ones(len(embeddings_metadata), dtype=bool)\n",
    "    \n",
    "    for i in range(len(embeddings_metadata)):\n",
    "        if not keep_mask[i]:  # Skip if already marked for removal\n",
    "            continue\n",
    "            \n",
    "        # Define window boundaries\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(embeddings_metadata), i + window_size + 1)\n",
    "        \n",
    "        # Compute similarities within window\n",
    "        similarities = np.dot(normalized[start:end], normalized[i])\n",
    "        \n",
    "        # Find indices with high similarity (excluding self)\n",
    "        for j, sim in enumerate(similarities):\n",
    "            actual_idx = start + j\n",
    "            if actual_idx != i and sim > similarity_threshold and keep_mask[actual_idx]:\n",
    "                keep_mask[actual_idx] = False\n",
    "    \n",
    "    # Return filtered embeddings\n",
    "    return [emb for emb, keep in zip(embeddings_metadata, keep_mask) if keep]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_metadata = []\n",
    "for token_embedding, token_index in zip(embedding_tokens, prompt_no_chat):\n",
    "    is_alpha = re.match(r'^[a-zA-Z\\s]+$', tokenizer.decode([token_index])) is not None\n",
    "\n",
    "\n",
    "    embedding_metadata.append({\n",
    "        \"embedding\": token_embedding,\n",
    "        \"magnitude\": np.linalg.norm(token_embedding),\n",
    "        \"token\": tokenizer.decode([token_index]),\n",
    "        \"index\": token_index,\n",
    "        \"is_alpha\": is_alpha\n",
    "\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_metadata_pruned = remove_similar_embeddings(embedding_metadata, window_size=20, similarity_threshold=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_indices = [x['index'] for x in embedding_metadata_pruned]\n",
    "pruned_user_prompt = tokenizer.decode(pruned_indices)\n",
    "print(\"Pruned Prompt: \", pruned_user_prompt)\n",
    "\n",
    "pct_pruned = round(1 - (len(embedding_metadata_pruned) / len(embedding_metadata)), 2)\n",
    "print(f\"{round(pct_pruned * 100)}% of tokens were pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_metadata_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both pruned and unpruned prompts on some examples\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    base_url=\"https://api.llama.com/compat/v1/\",\n",
    ")\n",
    "\n",
    "pruned_response = client.chat.completions.create(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": pruned_user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "original_response = client.chat.completions.create(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": chatbot_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_judge_prompt(prompt, response):\n",
    "    judge_prompt = f\"\"\"\n",
    "    You are an expert judge at reviewing the quality of responses to prompts. Given a prompt and response, please return a score between 0 and 100.\n",
    "\n",
    "Criteria:\n",
    "- Quality of response\n",
    "- Relevance to prompt\n",
    "- Creativity\n",
    "- Coherence\n",
    "- Style\n",
    "\n",
    "Scoring Guidelines:\n",
    "- 0-20: Poor\n",
    "- 21-40: Fair\n",
    "- 41-60: Good\n",
    "- 61-80: Excellent\n",
    "- 81-100: Perfect\n",
    "\n",
    "Prompt: \n",
    "{prompt}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "Please return your score within xml tags, following the format below:\n",
    "<reasoning>\n",
    "[explanation]\n",
    "</reasoning>\n",
    "<score>\n",
    "[score]\n",
    "</score>\n",
    "\n",
    "Example:\n",
    "<reasoning>\n",
    "The response is not relevant to the prompt.\n",
    "</reasoning>\n",
    "<score>\n",
    "0\n",
    "</score>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "judge_prompt = generate_judge_prompt(chatbot_prompt, pruned_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drh-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
