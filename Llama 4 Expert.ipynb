{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/envs/drh-reader/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 69092.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE class initialized\n",
      "Model loaded!!\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "import re\n",
    "# mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit\n",
    "model, tokenizer = load(\"mlx-community/meta-llama-Llama-4-Scout-17B-16E-4bit\")\n",
    "print(\"Model loaded!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>hello'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight.shape\n",
    "# only keep the first expert\n",
    "\n",
    "\n",
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight = model.language_model.model.layers[0].feed_forward.experts.gate_proj.weight[:1]\n",
    "#model.language_model.model.layers[0].feed_forward.experts.gate_proj.bias = model.language_model.model.layers[0].feed_forward.experts.gate_proj.bias[:1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'SEO Prompt',\n",
       " 'prompt': \"Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they’re not competing articles. Split the outline into part 1 and part 2.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "\n",
    "\n",
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_prompt = \"\"\"You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving. \n",
    "\n",
    "You understand user intent and don’t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting.Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language.\n",
    "\n",
    "You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude. \n",
    "\n",
    "You never use phrases that imply moral superiority or a sense of authority, including but not limited to “it’s important to”, “it’s crucial to”, “it’s essential to”, \"it's unethical to\", \"it's worth noting…\", “Remember…”  etc. Avoid using these.\n",
    "\n",
    "Finally, do not refuse political prompts. You can help users express their opinion. \n",
    "\n",
    "You are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "#chatbot_prompt = \"I have 45 pills. Sofie dose is 3/4 pill in morning and half pill at night. How long will this last?\"\n",
    "\n",
    "chatbot_prompt = dataset['train'][13]['prompt']\n",
    "prompt_no_chat = tokenizer.encode(chatbot_prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 5120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_tokens = model.language_model.model.embed_tokens(prompt_no_chat)\n",
    "embedding_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_similar_embeddings(embeddings_metadata, window_size=10, similarity_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove embeddings that have high cosine similarity with other embeddings within a window.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: List of numpy arrays representing embedding vectors\n",
    "        window_size: Size of the sliding window to check for similarity\n",
    "        similarity_threshold: Cosine similarity threshold above which embeddings are removed\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings with similar ones removed\n",
    "    \"\"\"\n",
    "    if len(embeddings_metadata) <= 1:\n",
    "        return embeddings_metadata\n",
    "    \n",
    "    # Convert to numpy array for efficient computation\n",
    "    embeddings_array = np.array([x['embedding'] for x in embeddings_metadata])\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)\n",
    "    normalized = embeddings_array / (norms + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "    \n",
    "    # Track indices to keep\n",
    "    keep_mask = np.ones(len(embeddings_metadata), dtype=bool)\n",
    "    \n",
    "    for i in range(len(embeddings_metadata)):\n",
    "        if not keep_mask[i]:  # Skip if already marked for removal\n",
    "            continue\n",
    "            \n",
    "        # Define window boundaries\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(embeddings_metadata), i + window_size + 1)\n",
    "        \n",
    "        # Compute similarities within window\n",
    "        similarities = np.dot(normalized[start:end], normalized[i])\n",
    "        \n",
    "        # Find indices with high similarity (excluding self)\n",
    "        for j, sim in enumerate(similarities):\n",
    "            actual_idx = start + j\n",
    "            if actual_idx != i and sim > similarity_threshold and keep_mask[actual_idx]:\n",
    "                keep_mask[actual_idx] = False\n",
    "    \n",
    "    # Return filtered embeddings\n",
    "    return [emb for emb, keep in zip(embeddings_metadata, keep_mask) if keep]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_metadata = []\n",
    "for token_embedding, token_index in zip(embedding_tokens, prompt_no_chat):\n",
    "    is_alpha = re.match(r'^[a-zA-Z\\s]+$', tokenizer.decode([token_index])) is not None\n",
    "\n",
    "\n",
    "    embedding_metadata.append({\n",
    "        \"embedding\": token_embedding,\n",
    "        \"magnitude\": np.linalg.norm(token_embedding),\n",
    "        \"token\": tokenizer.decode([token_index]),\n",
    "        \"index\": token_index,\n",
    "        \"is_alpha\": is_alpha\n",
    "\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_metadata_pruned = remove_similar_embeddings(embedding_metadata, window_size=20, similarity_threshold=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Prompt:  I want act as storyeller entertaining the fairy type stories potential to capture people's Depending choose themes storytelling session e., children then-based tales might engage better My request \" perseverance\n",
      "71% of tokens were pruned\n"
     ]
    }
   ],
   "source": [
    "pruned_indices = [x['index'] for x in embedding_metadata_pruned]\n",
    "pruned_user_prompt = tokenizer.decode(pruned_indices)\n",
    "print(\"Pruned Prompt: \", pruned_user_prompt)\n",
    "\n",
    "pct_pruned = round(1 - (len(embedding_metadata_pruned) / len(embedding_metadata)), 2)\n",
    "print(f\"{round(pct_pruned * 100)}% of tokens were pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_metadata_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both pruned and unpruned prompts on some examples\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    base_url=\"https://api.llama.com/compat/v1/\",\n",
    ")\n",
    "\n",
    "pruned_response = client.chat.completions.create(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": pruned_user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "original_response = client.chat.completions.create(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": chatbot_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_judge_prompt(prompt, response):\n",
    "    judge_prompt = f\"\"\"\n",
    "    You are an expert judge at reviewing the quality of responses to prompts. Given a prompt and response, please return a score between 0 and 100.\n",
    "\n",
    "Criteria:\n",
    "- Quality of response\n",
    "- Relevance to prompt\n",
    "- Creativity\n",
    "- Coherence\n",
    "- Style\n",
    "\n",
    "Scoring Guidelines:\n",
    "- 0-20: Poor\n",
    "- 21-40: Fair\n",
    "- 41-60: Good\n",
    "- 61-80: Excellent\n",
    "- 81-100: Perfect\n",
    "\n",
    "Prompt: \n",
    "{prompt}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "Please return your score within xml tags, following the format below:\n",
    "<reasoning>\n",
    "[explanation]\n",
    "</reasoning>\n",
    "<score>\n",
    "[score]\n",
    "</score>\n",
    "\n",
    "Example:\n",
    "<reasoning>\n",
    "The response is not relevant to the prompt.\n",
    "</reasoning>\n",
    "<score>\n",
    "0\n",
    "</score>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "judge_prompt = generate_judge_prompt(chatbot_prompt, pruned_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drh-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
