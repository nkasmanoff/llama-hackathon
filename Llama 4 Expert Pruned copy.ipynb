{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/envs/drh-reader/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 170174.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE class initialized\n",
      "Model loaded!!\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "from mlx_lm import load \n",
    "\n",
    "import re\n",
    "# mlx-community/Llama-4-Scout-17B-16E-Instruct-4bit\n",
    "model, tokenizer = load(\"mlx-community/meta-llama-Llama-4-Scout-17B-16E-4bit\",lazy=True)\n",
    "print(\"Model loaded!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active parameters: end=16,839,459,840\n",
      "Total parameters after pruning: 2,683,683,840\n"
     ]
    }
   ],
   "source": [
    "# prune experts\n",
    "from mlx.utils import tree_flatten  \n",
    "num_params = sum(v.size for _, v in tree_flatten(model.parameters()))\n",
    "# add commas to the number\n",
    "print(\"Active parameters:\", f\"end={num_params:,}\")\n",
    "\n",
    "for layer_idx, layer in enumerate(model.language_model.model.layers):\n",
    "    # print(f\"Processing layer {layer_idx}...\")\n",
    "    experts_group = layer.feed_forward.experts\n",
    "\n",
    "    components_to_prune = []\n",
    "    if hasattr(experts_group, 'gate_proj') and hasattr(experts_group, 'up_proj') and hasattr(experts_group, 'down_proj'):\n",
    "        components_to_prune.extend([experts_group.gate_proj, experts_group.up_proj, experts_group.down_proj])\n",
    "    elif hasattr(experts_group, 'fc1') and hasattr(experts_group, 'fc2'):\n",
    "        components_to_prune.extend([experts_group.fc1, experts_group.fc2])\n",
    "    else:\n",
    "        # print(f\"Warning: Layer {layer_idx} - experts_group (type: {type(experts_group)}) does not have expected proj/fc attributes.\")\n",
    "        continue\n",
    "\n",
    "    for comp_idx, component in enumerate(components_to_prune):\n",
    "        if not (hasattr(component, 'weight') and hasattr(component, 'scales') and hasattr(component, 'biases')):\n",
    "            # print(f\"  Skipping component {comp_idx} in layer {layer_idx} (type: {type(component)}), missing one or more of: weight, scales, biases (quantization parameters).\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"  Pruning component {comp_idx} (type: {type(component)}) in layer {layer_idx}.\")\n",
    "        # print(f\"    Old shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\n",
    "\n",
    "        component.weight = component.weight[0:1]\n",
    "        component.scales = component.scales[0:1]\n",
    "        component.biases = component.biases[0:1]  # Quantization biases\n",
    "\n",
    "        # print(f\"    New shapes: W={component.weight.shape}, S={component.scales.shape}, B_quant={component.biases.shape}\")\n",
    "\n",
    "        if hasattr(component, 'bias') and component.bias is not None:\n",
    "            # print(f\"    Old additive bias shape: {component.bias.shape}\")\n",
    "            component.bias = component.bias[0:1]  # Additive bias\n",
    "            # print(f\"    New additive bias shape: {component.bias.shape}\")\n",
    "        # else:\n",
    "            # print(f\"    Component {comp_idx} in layer {layer_idx} has no additive bias or it is None.\")\n",
    "\n",
    "    # The following lines were in the original cell for per-layer param count, kept commented\n",
    "    # layer_params = sum(v.size for _, v in tree_flatten(layer.parameters()))\n",
    "    # print(f\"Layer {layer_idx} parameters after pruning: {layer_params:,}\")\n",
    "    # print(f\"Finished processing layer {layer_idx}.\")\n",
    "\n",
    "num_params_after = sum(v.size for _, v in tree_flatten(model.parameters()))\n",
    "print(f\"Total parameters after pruning: {num_params_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated model parameters. Check memory footprint now.\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "mx.eval(model.parameters())\n",
    "print(\"Evaluated model parameters. Check memory footprint now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([200000, 200005, 1556, ..., 140680, 200006, 368], dtype=int32)\n",
      "<|begin_of_text|><|header_start|>user<|header_end|>\n",
      "\n",
      "Hello! Please tell me a joke<|eot|><|header_start|>assistant<|header_end|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Hello! Please tell me a joke\"\n",
    "\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = mx.array(prompt)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(prompt.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_tokens = 5\n",
    "for _ in range(max_tokens):\n",
    "    embedding_tokens = model.language_model.model.embed_tokens(prompt[None])\n",
    "    x = embedding_tokens\n",
    "    for layer in model.language_model.model.layers:\n",
    "        x = layer(x)\n",
    "    lm_head_output = model.language_model.lm_head(x)\n",
    "\n",
    "    new_token = lm_head_output[:,-1,:].tolist()\n",
    "    new_token = new_token[0].index(max(new_token[0]))\n",
    "    prompt = mx.array(prompt + [new_token])\n",
    "    print(tokenizer.decode(prompt.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token = logits[0].index(max(logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new token to prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drh-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
