{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349230e2-6cae-45b1-b50d-b3e01b3c96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/llama-models\n"
     ]
    }
   ],
   "source": [
    "cd ~/llama-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1728295-0e45-4f60-86d5-3deb062951b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama4 import *\n",
    "import torch\n",
    "import codecs\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Callable, Generator, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc667d1-9750-45db-94ee-25e661485b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826698ef-a562-4afe-a0e3-e35d06ba8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.checkpoint import maybe_reshard_state_dict\n",
    "from models.datatypes import GenerationResult, QuantizationMode\n",
    "from models.llama4.args import ModelArgs\n",
    "from models.llama4.chat_format import ChatFormat, RawContent, RawMessage\n",
    "from models.llama4.datatypes import LLMInput, MaskedEmbedding, TransformerInput\n",
    "# from models.llama4.model import Transformer\n",
    "from models.llama4.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2698e8f-97e4-46ed-a5db-720897a877ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/data/llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a2e7be3-ccef-4a8d-a2cd-bce5d3c273ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = 'cuda:0'\n",
    "torch.cuda.set_device(local_rank)\n",
    "\n",
    "ckpt_paths = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "\n",
    "max_seq_len = 128\n",
    "max_batch_size = 1\n",
    "world_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf943d5b-fc55-47ea-96bc-050ecf99a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    **params,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "tokenizer = Tokenizer.get_instance()\n",
    "\n",
    "model_args.vocab_size = tokenizer.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708d9cdc-9a1f-46f1-aa16-a4cf0e8ba28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_paths = np.array(sorted(ckpt_paths))\n",
    "map_location = 'cpu'\n",
    "mmap = True\n",
    "state_dicts = [torch.load(str(p), map_location=map_location, mmap=mmap) for p in ckpt_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10368bc8-230d-4100-88c3-72089f88856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def convert_moe_weights(state_dict: Dict[str, Any], num_experts: int) -> Dict[str, Any]:\n",
    "    routed_keys = _MOE_WEIGHT_ROW_KEY | _MOE_WEIGHT_COLUMN_KEY\n",
    "    routed_regex = re.compile(\"|\".join(routed_keys))\n",
    "    keys = list(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if routed_regex.search(key):\n",
    "            state_dict[key] = state_dict.pop(key).unflatten(0, (num_experts, -1)).squeeze(dim=0)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eca0890-9d99-4a13-a3a1-39989afdee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "_WEIGHT_ROW_KEY = {\n",
    "    \"feed_forward.w2\",\n",
    "    \"feed_forward.mlp.fc2\",\n",
    "    \"attention.wo\",\n",
    "    \"feed_forward.mlp.fc2_weight\",\n",
    "    \"feed_forward.w_out_shared_DF.weight\",\n",
    "    \"attn.wo.weight\",\n",
    "    \"mlp.c_proj.weight\",\n",
    "}\n",
    "_MOE_WEIGHT_ROW_KEY = {\"feed_forward.experts.(moe_w_in_eD_F|moe_w_swiglu_eD_F)\"}\n",
    "\n",
    "_WEIGHT_COLUMN_KEY = {\n",
    "    \"output\",\n",
    "    \"feed_forward.(w1|w3)\",\n",
    "    \"feed_forward.mlp.(fc1|fc3)\",\n",
    "    \"feed_forward.mlp.fc1_weight\",\n",
    "    \"attention.(wk|wq|wv|wqkv).weight\",\n",
    "    \"feed_forward.(w_in_shared_FD|w_swiglu_FD)\",\n",
    "    \"attn.(wk|wq|wv).weight\",\n",
    "    \"attn.(wk|wq|wv).bias\",\n",
    "    \"mlp.c_fc.weight\",\n",
    "    \"mlp.c_fc.bias\",\n",
    "    \"conv1._linear.weight\",\n",
    "    \"tok_embeddings.weight\",\n",
    "    \"vision_projection.weight\",\n",
    "}\n",
    "_MOE_WEIGHT_COLUMN_KEY = {\"feed_forward.experts.moe_w_out_eF_D\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b57c70-7fad-4748-9731-23a77470aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        do_reduce: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.do_reduce = do_reduce\n",
    "\n",
    "        # Replace ColumnParallelLinear/RowParallelLinear with nn.Linear\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        # If checkpoint has combined fc1 weights, split into w1 and w3\n",
    "        if prefix + \"mlp.fc1_weight\" in state_dict:\n",
    "            w1_w3 = state_dict.pop(prefix + \"mlp.fc1_weight\")\n",
    "            w1, w3 = w1_w3.chunk(2, dim=0)\n",
    "            state_dict[prefix + \"w1.weight\"] = w1\n",
    "            state_dict[prefix + \"w3.weight\"] = w3\n",
    "            state_dict[prefix + \"w2.weight\"] = state_dict.pop(prefix + \"mlp.fc2_weight\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, ..., dim]\n",
    "        x1 = F.linear(x, self.w1.weight)\n",
    "        x3 = F.linear(x, self.w3.weight)\n",
    "        x = F.silu(x1) * x3\n",
    "        out = F.linear(x, self.w2.weight)\n",
    "        # On single GPU, no need to reduce across model parallel region\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8cc06b-faf4-49c2-b3ba-68a19d288542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.llama4.args import MoEArgs\n",
    "# from models.llama4.ffn import FeedForward\n",
    "\n",
    "\n",
    "def divide_exact(numerator: int, denominator: int) -> int:\n",
    "    assert numerator % denominator == 0, f\"{numerator} is not divisible by {denominator}\"\n",
    "    return numerator // denominator\n",
    "\n",
    "\n",
    "class Experts(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_local_experts: int,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dtype = torch.get_default_dtype()\n",
    "        self.num_local_experts = num_local_experts\n",
    "        self.dim = dim\n",
    "\n",
    "        # Since we're on a single GPU, divide_factor = 1\n",
    "        divide_factor = 1\n",
    "\n",
    "        # w1: [e, D, hidden_dim]\n",
    "        self.w1: nn.Parameter = nn.Parameter(\n",
    "            torch.empty(\n",
    "                num_local_experts,\n",
    "                dim,\n",
    "                divide_exact(hidden_dim, divide_factor),\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # w2: [e, hidden_dim, D]\n",
    "        self.w2: nn.Parameter = nn.Parameter(\n",
    "            torch.empty(\n",
    "                num_local_experts,\n",
    "                divide_exact(hidden_dim, divide_factor),\n",
    "                dim,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # w3: [e, D, hidden_dim]\n",
    "        self.w3: nn.Parameter = nn.Parameter(\n",
    "            torch.empty(\n",
    "                num_local_experts,\n",
    "                dim,\n",
    "                divide_exact(hidden_dim, divide_factor),\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        self.prefix = prefix\n",
    "        # If checkpoint uses merged tensors, split them into w1, w2, w3\n",
    "        if prefix + \"moe_w_in_eD_F\" in state_dict:\n",
    "            e = self.num_local_experts\n",
    "            D = self.dim\n",
    "            state_dict[prefix + \"w1\"] = state_dict.pop(prefix + \"moe_w_in_eD_F\").view(e, D, -1)\n",
    "            state_dict[prefix + \"w2\"] = state_dict.pop(prefix + \"moe_w_out_eF_D\").view(e, -1, D)\n",
    "            state_dict[prefix + \"w3\"] = state_dict.pop(prefix + \"moe_w_swiglu_eD_F\").view(e, D, -1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        routed_in_egD: torch.Tensor,  # [e*G, D]\n",
    "    ) -> torch.Tensor:\n",
    "        e = self.num_local_experts\n",
    "        D = self.dim\n",
    "\n",
    "        # Reshape to [e, G, D]\n",
    "        x_egD = routed_in_egD.view(e, -1, D)\n",
    "\n",
    "        # Apply Swiglu for each expert\n",
    "        out_egD = self.batched_swiglu(x_egD, self.w1, self.w3, self.w2)\n",
    "        # Flatten back to [e*G, D]\n",
    "        out_egD = out_egD.view(-1, D)\n",
    "\n",
    "        return out_egD\n",
    "\n",
    "    def batched_swiglu(self, x: Tensor, w1: Tensor, w3: Tensor, w2: Tensor) -> Tensor:\n",
    "        # x: [e, G, D]; w1: [e, D, F]; w3: [e, D, F]; w2: [e, F, D]\n",
    "        middle_out_egF = F.silu(torch.bmm(x, w1)) * torch.bmm(x, w3)\n",
    "        return torch.bmm(middle_out_egF, w2)\n",
    "\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    - x_bsD: [batch_size, seq_len, D]\n",
    "    - router_DE: [D, E]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        ffn_dim_multiplier: float,\n",
    "        multiple_of: int,\n",
    "        moe_args: MoEArgs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.moe_args = moe_args\n",
    "\n",
    "        # Compute GMLP hidden dimension\n",
    "        hidden_dim_denom: float = 1.0\n",
    "        if moe_args.auto_scale_F:\n",
    "            hidden_dim_denom = moe_args.capacity_factor + 1.0\n",
    "\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        if moe_args.auto_scale_F:\n",
    "            hidden_dim = int(hidden_dim / hidden_dim_denom)\n",
    "        hidden_dim += -hidden_dim % multiple_of\n",
    "\n",
    "        num_local_experts: int = moe_args.num_experts\n",
    "        dtype: torch.dtype = torch.get_default_dtype()\n",
    "\n",
    "        # Create Experts module (all experts local on this GPU)\n",
    "        self.experts = Experts(\n",
    "            num_local_experts,\n",
    "            dim,\n",
    "            hidden_dim,\n",
    "        )\n",
    "\n",
    "        # Router logits: [D, E]\n",
    "        self.router_DE: nn.Parameter = nn.Parameter(torch.empty(dim, moe_args.num_experts, dtype=dtype))\n",
    "\n",
    "        # Shared expert (dense FFN) — no reduce needed\n",
    "        self.shared_expert = FeedForward(dim, hidden_dim, do_reduce=False)\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        # Remap shared expert weights if needed\n",
    "        if prefix + \"w_in_shared_FD.weight\" in state_dict:\n",
    "            state_dict[prefix + \"shared_expert.w1.weight\"] = state_dict.pop(prefix + \"w_in_shared_FD.weight\")\n",
    "            state_dict[prefix + \"shared_expert.w3.weight\"] = state_dict.pop(prefix + \"w_swiglu_FD.weight\")\n",
    "            state_dict[prefix + \"shared_expert.w2.weight\"] = state_dict.pop(prefix + \"w_out_shared_DF.weight\")\n",
    "\n",
    "    def forward(self, x_bsD: Tensor) -> Tensor:\n",
    "        bsz, slen, D = x_bsD.shape\n",
    "        # Flatten tokens: [bsz * slen, D]\n",
    "        x_aD = x_bsD.view(-1, D)\n",
    "        a = x_aD.shape[0]\n",
    "\n",
    "        # Compute router scores: [E, a]\n",
    "        router_scores: Tensor = torch.matmul(x_aD, self.router_DE).transpose(0, 1)\n",
    "\n",
    "        # Find top-k experts per token: router_indices_aK: [a, top_k], router_scores_aK: [a, top_k]\n",
    "        router_scores_aK, router_indices_aK = torch.topk(router_scores.transpose(0, 1), self.moe_args.top_k, dim=1)\n",
    "\n",
    "        # Zero out all but top-k scores\n",
    "        mask_full = torch.full_like(router_scores.transpose(0, 1), float(\"-inf\"))\n",
    "        mask_full = mask_full.scatter_(1, router_indices_aK, router_scores_aK)\n",
    "        router_scores = mask_full.transpose(0, 1)\n",
    "\n",
    "        # Indices for gathering tokens: [E, a]\n",
    "        router_indices = torch.arange(a, device=x_aD.device).view(1, -1).expand(router_scores.size(0), -1)\n",
    "\n",
    "        # Normalize scores with sigmoid\n",
    "        router_scores = torch.sigmoid(router_scores)\n",
    "\n",
    "        # Gather routed inputs: [E * G, D]  (where G = number of tokens per expert)\n",
    "        routed_in_EG_D: Tensor = torch.gather(\n",
    "            x_aD,\n",
    "            dim=0,\n",
    "            index=router_indices.reshape(-1, 1).expand(-1, D),\n",
    "        )\n",
    "        routed_in_EG_D = routed_in_EG_D * router_scores.reshape(-1, 1)\n",
    "\n",
    "        # Always apply shared expert first\n",
    "        out_aD = self.shared_expert(x_aD)\n",
    "\n",
    "        # Run local experts on detached routed inputs\n",
    "        routed_out_eg_D = self.experts(routed_in_EG_D.detach())\n",
    "\n",
    "        # Scatter-add expert outputs back into out_aD\n",
    "        router_indices_EG_D = router_indices.reshape(-1, 1).expand(-1, D)\n",
    "        out_aD.scatter_add_(dim=0, index=router_indices_EG_D, src=routed_out_eg_D.view(-1, D))\n",
    "\n",
    "        # On a single GPU, no need to reduce across parallel regions\n",
    "        # out_aD = reduce_from_model_parallel_region(out_aD)\n",
    "\n",
    "        # Reshape back to [bsz, slen, D]\n",
    "        return out_aD.view(bsz, slen, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4908c348-ccf9-474c-a1f2-e94823eb6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "\n",
    "from models.llama4.args import ModelArgs\n",
    "from models.llama4.model import Attention\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "\n",
    "\n",
    "class ColumnParallelConv2dPatch(nn.Module):\n",
    "    \"\"\"Conv2D Patching layer (single‐GPU version).\n",
    "    Arguments:\n",
    "        in_channels: Input channels.\n",
    "        out_channels: Output channels.\n",
    "        kernel_size: Size of convolution kernel.\n",
    "        stride (default 1): Stride for convolution.\n",
    "        bias (default False): Use bias in Conv2d.\n",
    "    Input: (bsz, in_channels, height, width)\n",
    "    Output: (bsz, num_tokens, out_channels)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]],\n",
    "        stride: Union[int, Tuple[int, int]],\n",
    "        bias: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self._unfold = nn.Unfold(kernel_size=kernel_size, stride=stride)\n",
    "        in_features = in_channels * kernel_size[0] * kernel_size[1]\n",
    "        self._linear = nn.Linear(in_features, out_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [bsz, in_channels, height, width]\n",
    "        x = self._unfold(x)                       # [bsz, in_channels * k_h * k_w, num_tokens]\n",
    "        x = x.permute(0, 2, 1)                    # [bsz, num_tokens, in_channels * k_h * k_w]\n",
    "        x = self._linear(x)                       # [bsz, num_tokens, out_channels]\n",
    "        return x\n",
    "\n",
    "\n",
    "class _FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        dropout: float,\n",
    "        act_layer: Callable = nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Single‐GPU linear layers\n",
    "        self.c_fc = nn.Linear(dim, hidden_dim, bias=True)\n",
    "        self.c_proj = nn.Linear(hidden_dim, dim, bias=True)\n",
    "        self.non_linearity = act_layer()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = self.c_fc(x)                     # [*, hidden_dim]\n",
    "        hidden = self.non_linearity(hidden)\n",
    "        hidden = F.dropout(hidden, p=self.dropout, training=self.training)\n",
    "        hidden = self.c_proj(hidden)              # [*, dim]\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class _TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_head: int,\n",
    "        max_batch_size: int,\n",
    "        max_seq_len: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        act_layer: Callable = nn.GELU,\n",
    "        gated: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.n_heads = n_head\n",
    "        self.head_dim = d_model // self.n_heads\n",
    "\n",
    "        attn_args = ModelArgs(\n",
    "            dim=d_model,\n",
    "            head_dim=self.head_dim,\n",
    "            n_heads=self.n_heads,\n",
    "            n_kv_heads=self.n_heads,\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.attn = Attention(attn_args, use_rope=True, use_qk_norm=False, add_bias=True)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = _FeedForward(\n",
    "            dim=d_model,\n",
    "            hidden_dim=int(mlp_ratio * d_model),\n",
    "            dropout=0.0,\n",
    "            act_layer=act_layer,\n",
    "        )\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.gated = gated\n",
    "        if gated:\n",
    "            self.gate_attn = nn.Parameter(torch.zeros(1))\n",
    "            self.gate_ffn = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def attention(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        freq_cis: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        return self.attn(x=x, start_pos=0, freqs_cis=freq_cis)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        freq_cis: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        _gate_attn = 1 if not self.gated else self.gate_attn.tanh()\n",
    "        _gate_ffn = 1 if not self.gated else self.gate_ffn.tanh()\n",
    "\n",
    "        x = x + _gate_attn * self.attention(self.ln_1(x), freq_cis=freq_cis)\n",
    "        x = x + _gate_ffn * self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class _Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        max_batch_size: int,\n",
    "        max_seq_len: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        act_layer: Callable = nn.GELU,\n",
    "        gated: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.resblocks = nn.ModuleList(\n",
    "            [\n",
    "                _TransformerBlock(\n",
    "                    d_model=dim,\n",
    "                    n_head=heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    act_layer=act_layer,\n",
    "                    gated=gated,\n",
    "                    max_batch_size=max_batch_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                )\n",
    "                for _ in range(layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_intermediate=None, mask=None, freq_cis=None):\n",
    "        out = []\n",
    "        for idx, r in enumerate(self.resblocks):\n",
    "            if return_intermediate is not None and idx in return_intermediate:\n",
    "                out.append(x)\n",
    "            x = r(x, mask=mask, freq_cis=freq_cis)\n",
    "        if return_intermediate is not None:\n",
    "            return x, torch.stack(out, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PackingIndex:\n",
    "    Z = 0  # Z (time) coordinate of the token in the original sample\n",
    "    Y = 1  # Y (height) coordinate of the token in the original sample\n",
    "    X = 2  # X (width) coordinate of the token in the original sample\n",
    "    TIME = 3  # Total number of time units (frames) in the original sample\n",
    "    HEIGHT = 4  # Height of the original sample\n",
    "    WIDTH = 5  # Width of the original sample\n",
    "    IDX = 6  # Full index of the token in the original sample (x + y * w + z * w * h)\n",
    "    BATCH_IDX = 7  # Which batch element this token belongs to\n",
    "    NUM_METADATA = 8\n",
    "    ID_CLS_TOKEN = -2\n",
    "    ID_PAD_TOKEN = -1\n",
    "\n",
    "\n",
    "ENCODER_MAX_BATCH_SIZE = 32\n",
    "ENCODER_MAX_SEQ_LEN = 1024\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: Tuple[int, int],\n",
    "        patch_size: Tuple[int, int],\n",
    "        dim: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        mlp_ratio: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (\n",
    "            self.image_size[0] // self.patch_size[0],\n",
    "            self.image_size[1] // self.patch_size[1],\n",
    "        )\n",
    "\n",
    "        # Replace ColumnParallelConv2dPatch with single‐GPU version\n",
    "        self.conv1 = ColumnParallelConv2dPatch(\n",
    "            in_channels=3,\n",
    "            out_channels=dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        scale = dim**-0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(dim))\n",
    "        self.positional_embedding_vlm = nn.Parameter(\n",
    "            scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, dim)\n",
    "        )\n",
    "\n",
    "        self.ln_pre = LayerNorm(dim)\n",
    "        self.ln_post = LayerNorm(dim)\n",
    "\n",
    "        # Build transformer (single‐GPU)\n",
    "        self.transformer = _Transformer(\n",
    "            dim,\n",
    "            layers,\n",
    "            heads,\n",
    "            ENCODER_MAX_BATCH_SIZE,\n",
    "            ENCODER_MAX_SEQ_LEN,\n",
    "            mlp_ratio,\n",
    "            act_layer=nn.GELU,\n",
    "        )\n",
    "\n",
    "        # Compute packed indices for positional embedding\n",
    "        image_h, image_w = self.image_size\n",
    "        patch_h, patch_w = self.patch_size\n",
    "        idx_h, idx_w = image_h // patch_h, image_w // patch_w\n",
    "\n",
    "        img_idx = torch.arange(image_h * image_w // (patch_h * patch_w), dtype=torch.int32)\n",
    "        img_idx = img_idx.reshape(idx_h * idx_w, 1)\n",
    "        img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)\n",
    "        img_idx[-1, -1] = PackingIndex.ID_CLS_TOKEN\n",
    "\n",
    "        packed_img_idx = torch.empty(\n",
    "            img_idx.shape[0],\n",
    "            img_idx.shape[1],\n",
    "            PackingIndex.NUM_METADATA - 1,\n",
    "            dtype=torch.int32,\n",
    "        )\n",
    "        packed_img_idx[:, :, PackingIndex.Y] = img_idx // idx_w\n",
    "        packed_img_idx[:, :, PackingIndex.X] = img_idx % idx_w\n",
    "        packed_img_idx[:, :, PackingIndex.HEIGHT].fill_(idx_h)\n",
    "        packed_img_idx[:, :, PackingIndex.WIDTH].fill_(idx_w)\n",
    "        packed_img_idx[:, :, PackingIndex.IDX] = img_idx\n",
    "        packed_img_idx = packed_img_idx.reshape(1, -1, PackingIndex.NUM_METADATA - 1)\n",
    "        self.packed_img_idx = packed_img_idx  # for positional embedding load hook\n",
    "\n",
    "        # Compute RoPE frequencies\n",
    "        rope_freq = self.get_rope_freqs(dim // heads // 2)\n",
    "        freqs_x = self.compute_rope_freqs(rope_freq, packed_img_idx[:, :, PackingIndex.X] + 1)\n",
    "        freqs_y = self.compute_rope_freqs(rope_freq, packed_img_idx[:, :, PackingIndex.Y] + 1)\n",
    "        freqs = torch.cat([freqs_x, freqs_y], dim=-1).float().contiguous()[..., ::2]\n",
    "        freqs = freqs.masked_fill(packed_img_idx[:, :, PackingIndex.IDX, None] < 0, 0)\n",
    "        self.freq_cis = torch.view_as_complex(torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1))\n",
    "        self.freq_cis = self.freq_cis.squeeze(0)\n",
    "\n",
    "        # On a single GPU, n_heads is just heads (no world_size divide)\n",
    "        self.n_heads = heads\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def get_rope_freqs(self, dim, theta=10000):\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        return freqs\n",
    "\n",
    "    @torch.amp.autocast(\"cuda\", enabled=False)\n",
    "    def compute_rope_freqs(self, freqs, t):\n",
    "        freqs = einsum(\"..., f -> ... f\", t.type(freqs.dtype), freqs)\n",
    "        freqs = freqs.repeat_interleave(2, dim=-1)\n",
    "        return freqs\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool = True,\n",
    "        missing_keys: List[str] = None,\n",
    "        unexpected_keys: List[str] = None,\n",
    "        error_msgs: List[str] = None,\n",
    "        return_state_dict: bool = False,\n",
    "    ) -> None:\n",
    "        orig_pos_embed = state_dict.get(prefix + \"positional_embedding\")\n",
    "        if orig_pos_embed is not None and orig_pos_embed.shape[-2:] != self.positional_embedding_vlm.shape[-2:]:\n",
    "            raise ValueError(\n",
    "                f\"Positional embedding shape {orig_pos_embed.shape} does not match expected shape {self.positional_embedding_vlm.shape}\"\n",
    "            )\n",
    "\n",
    "        batch_size, token_per_image, _ = self.packed_img_idx.shape\n",
    "        idx = self.packed_img_idx.reshape(batch_size * token_per_image, 1, -1)\n",
    "        total_windows, window_size, _ = idx.shape\n",
    "\n",
    "        grid = (\n",
    "            (idx[:, :, [PackingIndex.X, PackingIndex.Y]] / idx[:, :, [PackingIndex.WIDTH, PackingIndex.HEIGHT]]) * 2 - 1\n",
    "        )[None, ...]\n",
    "\n",
    "        if orig_pos_embed is not None:\n",
    "            posemb = (\n",
    "                orig_pos_embed[1:].view(1, self.grid_size[0], self.grid_size[1], -1)\n",
    "                .permute(0, 3, 1, 2)\n",
    "                .contiguous()\n",
    "            )\n",
    "            posemb = posemb.to(device=grid.device, dtype=grid.dtype)\n",
    "            sample = F.grid_sample(\n",
    "                posemb, grid, padding_mode=\"zeros\"\n",
    "            )\n",
    "            sample = sample.view(-1, total_windows, window_size).permute(1, 2, 0).contiguous()\n",
    "            sample = torch.where(\n",
    "                idx[:, :, PackingIndex.IDX, None] == PackingIndex.ID_CLS_TOKEN,\n",
    "                orig_pos_embed[0].view(1, 1, -1).to(device=sample.device, dtype=sample.dtype),\n",
    "                sample,\n",
    "            )\n",
    "            new_pos_embed = sample.reshape(batch_size, token_per_image, -1)\n",
    "            state_dict[prefix + \"positional_embedding_vlm\"] = new_pos_embed.squeeze(0)\n",
    "\n",
    "        if return_state_dict:\n",
    "            return state_dict\n",
    "\n",
    "    def apply_class_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        cls = self.class_embedding.to(x.dtype) + torch.zeros(\n",
    "            x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device\n",
    "        )\n",
    "        return torch.cat([x, cls], dim=1)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        if images.ndim == 5:\n",
    "            num_concurrent_media = 1\n",
    "            bsz, num_chunks, nch, h, w = images.shape\n",
    "        else:\n",
    "            bsz, num_concurrent_media, num_chunks, nch, h, w = images.shape\n",
    "\n",
    "        images = images.reshape(bsz * num_concurrent_media * num_chunks, nch, h, w)\n",
    "        x = self.conv1(images)  # [*, num_patches, dim]\n",
    "        _, ntok, dim = x.shape\n",
    "        x = x.reshape(bsz * num_concurrent_media * num_chunks, ntok, dim)\n",
    "\n",
    "        x = self.apply_class_embedding(x)\n",
    "        ntok += 1\n",
    "\n",
    "        if self.positional_embedding_vlm is not None:\n",
    "            x = x + self.positional_embedding_vlm.to(x.dtype)\n",
    "\n",
    "        x = x.reshape(bsz * num_concurrent_media, num_chunks, ntok, dim)\n",
    "        x = self.ln_pre(x)\n",
    "        x = x.view(bsz * num_concurrent_media, -1, dim)\n",
    "        freq_cis = self.freq_cis.to(images.device)\n",
    "\n",
    "        tf_output = self.transformer(x, freq_cis=freq_cis)\n",
    "\n",
    "        int_x = None\n",
    "        if isinstance(tf_output, tuple):\n",
    "            x, int_x = tf_output\n",
    "        else:\n",
    "            x = tf_output\n",
    "        x = self.ln_post(x)\n",
    "\n",
    "        x = x[:, :-1, :]  # remove cls token\n",
    "\n",
    "        if int_x is not None:\n",
    "            int_x = int_x[:, :-1, :, :].reshape(bsz * num_concurrent_media, ntok - 1, -1)\n",
    "            x = torch.cat([x, int_x], dim=-1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9c40af-2ded-4ef4-a9e3-e99743334ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Callable, Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.llama4.args import VisionArgs\n",
    "# from .encoder import VisionEncoder\n",
    "\n",
    "\n",
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, ps_ratio):\n",
    "        super().__init__()\n",
    "        self.ps_ratio = ps_ratio\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, N, C], N = number of patches\n",
    "        assert self.ps_ratio is not None, \"ps_ratio is required for pixel shuffle\"\n",
    "        assert x.dim() == 3, \"pixel shuffle requires encoded patches [B, N, C]\"\n",
    "        hh = ww = int(math.sqrt(x.shape[1]))\n",
    "        x = x.reshape(x.shape[0], hh, ww, -1)  # [B, H, W, C]\n",
    "        x = pixel_shuffle_op(x, ps_ratio=self.ps_ratio)\n",
    "        pixel_shuffle_patches = x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "        return pixel_shuffle_patches\n",
    "\n",
    "\n",
    "def pixel_shuffle_op(input_x: torch.Tensor, ps_ratio: float) -> torch.Tensor:\n",
    "    # input_x: [N, W, H, C]\n",
    "    n, w, h, c = input_x.size()\n",
    "    input_x = input_x.view(n, w, int(h * ps_ratio), int(c / ps_ratio))\n",
    "    input_x = input_x.permute(0, 2, 1, 3).contiguous()\n",
    "    input_x = input_x.view(\n",
    "        n,\n",
    "        int(h * ps_ratio),\n",
    "        int(w * ps_ratio),\n",
    "        int(c / (ps_ratio * ps_ratio)),\n",
    "    )\n",
    "    input_x = input_x.permute(0, 2, 1, 3).contiguous()\n",
    "    return input_x\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        act_layer: Callable = nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear instead of ColumnParallelLinear/RowParallelLinear\n",
    "        self.c_fc = nn.Linear(dim, hidden_dim, bias=bias)\n",
    "        self.c_proj = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "        self.non_linearity = act_layer()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = self.c_fc(x)                     # [*, hidden_dim]\n",
    "        hidden = self.non_linearity(hidden)\n",
    "        hidden = F.dropout(hidden, p=self.dropout, training=self.training)\n",
    "        hidden = self.c_proj(hidden)              # [*, hidden_dim]\n",
    "        return self.non_linearity(hidden)\n",
    "\n",
    "\n",
    "class PixelShuffleMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ps_ratio: float,\n",
    "        input_dim: int,\n",
    "        output_dim: int = 4096,\n",
    "        add_fc: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pixel_shuffle = PixelShuffle(ps_ratio)\n",
    "        self.mlp = SimpleMLP(\n",
    "            int(input_dim // (ps_ratio**2)),\n",
    "            output_dim,\n",
    "            bias=False,\n",
    "            dropout=0.0,\n",
    "            act_layer=nn.GELU,\n",
    "        )\n",
    "        self.fc = nn.Identity()\n",
    "        if add_fc:\n",
    "            # Replace ColumnParallelLinear with nn.Linear\n",
    "            self.fc = nn.Linear(output_dim, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, encoded_patches: torch.Tensor) -> torch.Tensor:\n",
    "        # encoded_patches: [B, N, C]\n",
    "        encoded_patches = self.pixel_shuffle(encoded_patches)  # [B, N', C]\n",
    "        return self.fc(self.mlp(encoded_patches))               # [B, N', output_dim]\n",
    "\n",
    "\n",
    "class VisionEmbeddings(nn.Module):\n",
    "    def __init__(self, args: VisionArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        image_size = args.image_size\n",
    "        patch_size = args.patch_size\n",
    "        self.vision_encoder = VisionEncoder(\n",
    "            image_size=(image_size.height, image_size.width),\n",
    "            patch_size=(patch_size.height, patch_size.width),\n",
    "            dim=args.dim,\n",
    "            layers=args.n_layers,\n",
    "            heads=args.n_heads,\n",
    "            mlp_ratio=args.mlp_ratio,\n",
    "        )\n",
    "        self.vision_encoder = self.vision_encoder.to(torch.bfloat16)\n",
    "        self.vision_adapter = PixelShuffleMLP(\n",
    "            ps_ratio=args.pixel_shuffle_ratio,\n",
    "            input_dim=args.dim,\n",
    "            output_dim=args.output_dim,\n",
    "        )\n",
    "\n",
    "        self.output_dim = args.output_dim\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool = True,\n",
    "        missing_keys: List[str] = None,\n",
    "        unexpected_keys: List[str] = None,\n",
    "        error_msgs: List[str] = None,\n",
    "        return_state_dict: bool = False,\n",
    "    ) -> None:\n",
    "        original_sd = self.state_dict()\n",
    "        for k in state_dict:\n",
    "            if (\n",
    "                k.startswith(prefix)\n",
    "                and len(state_dict[k].shape) == 1\n",
    "                and state_dict[k].shape[0] == 0\n",
    "            ):\n",
    "                state_dict[k] = state_dict[k].reshape(\n",
    "                    original_sd[k[len(prefix) :]].shape\n",
    "                )\n",
    "\n",
    "    def _get_empty_sequence(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.zeros(\n",
    "            h.shape[0],\n",
    "            h.shape[1],\n",
    "            self.output_dim,\n",
    "            device=h.device,\n",
    "            dtype=h.dtype,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_batch: List[List[torch.Tensor]],\n",
    "        image_mask: torch.Tensor,\n",
    "        h_ref: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Flatten all images in batch\n",
    "        images_flattened = [image for sample in image_batch for image in sample]\n",
    "        images_flattened = torch.vstack(images_flattened).unsqueeze(1).to(h_ref.dtype).to(h_ref.device)\n",
    "        # Encode patches\n",
    "        embedding = self.vision_encoder(images_flattened)           # [sum_chunks, num_patches, dim]\n",
    "        # Project via pixel‐shuffle + MLP\n",
    "        projected_embedding = self.vision_adapter(embedding)       # [sum_chunks, num_tokens, output_dim]\n",
    "\n",
    "        h_image = self._get_empty_sequence(h_ref)                  # [B, T, output_dim]\n",
    "        return scatter_embeddings(image_batch, image_mask, h_image, projected_embedding)\n",
    "\n",
    "\n",
    "def scatter_embeddings(\n",
    "    image_batch: List[List[torch.Tensor]],\n",
    "    image_mask: torch.Tensor,\n",
    "    h_image: torch.Tensor,\n",
    "    encoded_patches_proj: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    # Determine number of chunks per sample\n",
    "    num_images_per_sequence = [\n",
    "        sum(image.size(0) for image in sample_images) for sample_images in image_batch\n",
    "    ]\n",
    "\n",
    "    assert not torch.isnan(encoded_patches_proj).any()\n",
    "    assert sum(num_images_per_sequence) == encoded_patches_proj.size(0), (\n",
    "        f\"{sum(num_images_per_sequence)=} != {encoded_patches_proj.shape=}\"\n",
    "    )\n",
    "\n",
    "    encoded_patches_list = encoded_patches_proj.split(num_images_per_sequence, dim=0)\n",
    "    for index in range(h_image.size(0)):\n",
    "        encoded_patches_per_sample = encoded_patches_list[index]\n",
    "        sample_image_mask = image_mask[index]\n",
    "\n",
    "        if encoded_patches_per_sample.numel() == 0:\n",
    "            continue\n",
    "        encoded_patches_per_sample = encoded_patches_per_sample.contiguous().view(\n",
    "            -1, encoded_patches_per_sample.size(-1)\n",
    "        )\n",
    "\n",
    "        n_tokens_to_fill = sample_image_mask.sum()\n",
    "        assert n_tokens_to_fill <= encoded_patches_per_sample.size(0)\n",
    "\n",
    "        h_image[index].masked_scatter_(\n",
    "            sample_image_mask.expand(-1, h_image.size(-1)),\n",
    "            encoded_patches_per_sample[:n_tokens_to_fill],\n",
    "        )\n",
    "\n",
    "    return h_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4cb1e8-d6b8-410f-af23-a7edd708620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# from models.llama4.vision.embedding import VisionEmbeddings\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from models.llama4.args import ModelArgs\n",
    "from models.llama4.datatypes import TransformerInput, TransformerOutput\n",
    "# from models.llama4.ffn import FeedForward\n",
    "# from models.llama4.moe import MoE\n",
    "\n",
    "\n",
    "def rmsnorm(x, eps):\n",
    "    def _norm(y):\n",
    "        return y * torch.rsqrt(y.pow(2).mean(-1, keepdim=True) + eps)\n",
    "\n",
    "    return _norm(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return rmsnorm(x, self.eps) * self.weight\n",
    "\n",
    "\n",
    "def apply_scaling(freqs: torch.Tensor, scale_factor: float, high_freq_factor: float):\n",
    "    low_freq_factor = 1\n",
    "    old_context_len = 8192  # original llama3 length\n",
    "\n",
    "    low_freq_wavelen = old_context_len / low_freq_factor\n",
    "    high_freq_wavelen = old_context_len / high_freq_factor\n",
    "    new_freqs = []\n",
    "    for freq in freqs:\n",
    "        wavelen = 2 * math.pi / freq\n",
    "        if wavelen < high_freq_wavelen:\n",
    "            new_freqs.append(freq)\n",
    "        elif wavelen > low_freq_wavelen:\n",
    "            new_freqs.append(freq / scale_factor)\n",
    "        else:\n",
    "            assert low_freq_wavelen != high_freq_wavelen\n",
    "            smooth = (old_context_len / wavelen - low_freq_factor) / (\n",
    "                high_freq_factor - low_freq_factor\n",
    "            )\n",
    "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
    "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(\n",
    "    dim: int,\n",
    "    end: int,\n",
    "    theta: float,\n",
    "    use_scaled: bool,\n",
    "    scale_factor: float,\n",
    "    high_freq_factor: float,\n",
    "):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    if use_scaled:\n",
    "        freqs = apply_scaling(freqs, scale_factor, high_freq_factor)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # This module is now single‐GPU/CPU only (no model parallelism).\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: ModelArgs,\n",
    "        use_qk_norm: bool,\n",
    "        use_rope: bool,\n",
    "        add_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_rope = use_rope\n",
    "        self.use_qk_norm = use_qk_norm\n",
    "        # For attention temperature tuning\n",
    "        self.attn_temperature_tuning = args.attn_temperature_tuning\n",
    "        self.floor_scale = args.floor_scale\n",
    "        self.attn_scale = args.attn_scale\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        self.n_local_heads = self.n_heads  # no model parallel split\n",
    "        self.n_local_kv_heads = self.n_kv_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Replaced ColumnParallelLinear with nn.Linear\n",
    "        self.wq = nn.Linear(args.dim, self.n_heads * self.head_dim, bias=add_bias)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=add_bias)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=add_bias)\n",
    "\n",
    "        # Replaced RowParallelLinear with nn.Linear\n",
    "        self.wo = nn.Linear(self.n_heads * self.head_dim, args.dim, bias=add_bias)\n",
    "\n",
    "        # Caching buffers remain the same (will reside on whichever device the model is on)\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "\n",
    "        self.norm_eps = args.norm_eps\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        # Preserve any pretrained wqkv splitting logic if needed\n",
    "        if prefix + \"wqkv.weight\" in state_dict:\n",
    "            wqkv = state_dict.pop(prefix + \"wqkv.weight\")\n",
    "            d, r = divmod(wqkv.shape[0], self.n_heads + 2 * self.n_kv_heads)\n",
    "            if r != 0:\n",
    "                raise ValueError(\n",
    "                    f\"shape={tuple(wqkv.shape)} is not divisible by \"\n",
    "                    f\"n_heads ({self.n_heads}) + 2 * n_kv_heads ({self.n_kv_heads})\"\n",
    "                )\n",
    "            wq, wk, wv = wqkv.split(\n",
    "                [d * self.n_heads, d * self.n_kv_heads, d * self.n_kv_heads], dim=0\n",
    "            )\n",
    "            state_dict[prefix + \"wq.weight\"] = wq\n",
    "            state_dict[prefix + \"wk.weight\"] = wk\n",
    "            state_dict[prefix + \"wv.weight\"] = wv\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq = self.wq(x)\n",
    "        xk = self.wk(x)\n",
    "        xv = self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        if self.use_rope:\n",
    "            xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        if self.use_qk_norm:\n",
    "            xq = rmsnorm(xq, self.norm_eps)\n",
    "            xk = rmsnorm(xk, self.norm_eps)\n",
    "\n",
    "        # Attention temperature tuning (NoPE layers) remains unchanged\n",
    "        if self.attn_temperature_tuning and not self.use_rope:\n",
    "            seq_positions = torch.arange(\n",
    "                start_pos, start_pos + seqlen, device=xq.device, dtype=torch.float32\n",
    "            )\n",
    "            attn_scales = (\n",
    "                torch.log(torch.floor((seq_positions + 1.0) / self.floor_scale) + 1.0)\n",
    "                * self.attn_scale\n",
    "                + 1.0\n",
    "            )\n",
    "            attn_scales = attn_scales.view(1, seqlen, 1, 1)\n",
    "            xq = xq * attn_scales\n",
    "\n",
    "        # Ensure cache is on correct device\n",
    "        self.cache_k = self.cache_k.to(xq.device)\n",
    "        self.cache_v = self.cache_v.to(xq.device)\n",
    "\n",
    "        # Write into cache\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        xk = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        xv = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq, xk, xv = [t.transpose(1, 2) for t in (xq, xk, xv)]\n",
    "        xk = xk.repeat_interleave(self.n_rep, dim=1)\n",
    "        xv = xv.repeat_interleave(self.n_rep, dim=1)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            xq, xk, xv, attn_mask=mask, dropout_p=0.0\n",
    "        )\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        output = self.wo(attn_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads if args.head_dim is None else args.head_dim\n",
    "\n",
    "        self.is_nope_layer = (\n",
    "            args.nope_layer_interval is not None\n",
    "            and (layer_id + 1) % args.nope_layer_interval == 0\n",
    "        )\n",
    "\n",
    "        use_rope = not self.is_nope_layer\n",
    "        use_qk_norm = args.use_qk_norm and not self.is_nope_layer\n",
    "\n",
    "        self.attention = Attention(args, use_rope=use_rope, use_qk_norm=use_qk_norm)\n",
    "\n",
    "        if args.moe_args and (layer_id + 1) % args.moe_args.interleave_moe_layer_step == 0:\n",
    "            self.feed_forward = MoE(\n",
    "                dim=args.dim,\n",
    "                hidden_dim=int(args.ffn_exp * args.dim),\n",
    "                ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "                multiple_of=args.multiple_of,\n",
    "                moe_args=args.moe_args,\n",
    "            )\n",
    "        else:\n",
    "            hidden_dim = int(4 * args.dim)\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            if args.ffn_dim_multiplier is not None:\n",
    "                hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "            hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "            self.feed_forward = FeedForward(dim=args.dim, hidden_dim=hidden_dim)\n",
    "\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        if prefix + \"attention.wqkv.layer_norm_weight\" in state_dict:\n",
    "            state_dict[prefix + \"attention_norm.weight\"] = state_dict.pop(\n",
    "                prefix + \"attention.wqkv.layer_norm_weight\"\n",
    "            )\n",
    "\n",
    "        if prefix + \"feed_forward.mlp.layer_norm_weight\" in state_dict:\n",
    "            state_dict[prefix + \"ffn_norm.weight\"] = state_dict.pop(\n",
    "                prefix + \"feed_forward.mlp.layer_norm_weight\"\n",
    "            )\n",
    "        elif prefix + \"feed_forward.norm.weight\" in state_dict:\n",
    "            state_dict[prefix + \"ffn_norm.weight\"] = state_dict.pop(\n",
    "                prefix + \"feed_forward.norm.weight\"\n",
    "            )\n",
    "\n",
    "        for k in (\"feed_forward.experts.mlp\", \"feed_forward.mlp_shared\", \"attention.wo\", \"attention.wqkv\"):\n",
    "            if prefix + k + \"._extra_state\" in state_dict:\n",
    "                state_dict.pop(prefix + k + \"._extra_state\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        global_attn_mask: Optional[torch.Tensor],\n",
    "        local_attn_mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        # Use global mask if NoPE or if chunked local attention is disabled\n",
    "        if self.is_nope_layer or local_attn_mask is None:\n",
    "            mask = global_attn_mask\n",
    "        else:\n",
    "            mask = local_attn_mask\n",
    "\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        # Replaced VocabParallelEmbedding with nn.Embedding\n",
    "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, args))\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # Replaced ColumnParallelLinear with nn.Linear\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            args.dim // args.n_heads,\n",
    "            args.max_seq_len * 2,\n",
    "            args.rope_theta,\n",
    "            args.use_scaled_rope,\n",
    "            args.rope_scaling_factor,\n",
    "            args.rope_high_freq_factor,\n",
    "        )\n",
    "\n",
    "        vision_args = self.args.vision_args\n",
    "        if vision_args:\n",
    "            # circular import otherwise until we refactor out Attention\n",
    "            # from .vision.embedding import VisionEmbeddings\n",
    "\n",
    "            self.vision_embeddings = VisionEmbeddings(vision_args)\n",
    "            # Replaced ColumnParallelLinear with nn.Linear\n",
    "            self.vision_projection = nn.Linear(vision_args.output_dim, args.dim, bias=False)\n",
    "\n",
    "        self._register_load_state_dict_pre_hook(self.load_hook)\n",
    "\n",
    "    def load_hook(\n",
    "        self,\n",
    "        state_dict: Dict[str, Any],\n",
    "        prefix: str,\n",
    "        local_metadata: Dict[str, Any],\n",
    "        strict: bool,\n",
    "        missing_keys: List[str],\n",
    "        unexpected_keys: List[str],\n",
    "        error_msgs: List[str],\n",
    "    ) -> None:\n",
    "        if prefix + \"rope.freqs\" in state_dict:\n",
    "            state_dict.pop(prefix + \"rope.freqs\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, model_input: TransformerInput) -> TransformerOutput:\n",
    "        tokens = model_input.tokens\n",
    "        start_pos = model_input.tokens_position\n",
    "        assert isinstance(start_pos, int), (\n",
    "            \"This implementation does not support different start positions per batch item\"\n",
    "        )\n",
    "\n",
    "        bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        if image_embedding := model_input.image_embedding:\n",
    "            h_image = self.vision_projection(image_embedding.embedding)\n",
    "            h = h * ~image_embedding.mask + h_image * image_embedding.mask\n",
    "\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        global_attn_mask, local_attn_mask = None, None\n",
    "        if seqlen > 1:\n",
    "            global_attn_mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            global_attn_mask = torch.triu(global_attn_mask, diagonal=1).type_as(h)\n",
    "\n",
    "            # Handle MPS bug where triu produces NaNs instead of 0\n",
    "            if global_attn_mask.device.type == torch.device(\"mps\").type:\n",
    "                global_attn_mask = torch.nan_to_num(global_attn_mask, nan=0.0)\n",
    "\n",
    "            if chunk_size := self.args.attention_chunk_size:\n",
    "                local_attn_mask = create_chunked_attention_mask(seqlen, chunk_size, tokens.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, global_attn_mask, local_attn_mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "\n",
    "        return TransformerOutput(logits=output)\n",
    "\n",
    "\n",
    "def create_chunked_attention_mask(seq_len: int, attention_chunk_size: int, device: torch.device) -> torch.Tensor:\n",
    "    block_pos = torch.abs(\n",
    "        (torch.arange(seq_len).unsqueeze(0) // attention_chunk_size)\n",
    "        - (torch.arange(seq_len).unsqueeze(1) // attention_chunk_size)\n",
    "    )\n",
    "    token_pos = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)\n",
    "    mask = (block_pos == 0) & (token_pos <= 0)\n",
    "    return mask.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4e49374-a7f4-4fe0-a543-a03ba4e9a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_num_experts = 16\n",
    "state_dicts = [convert_moe_weights(d, moe_num_experts) for d in state_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb5c380-63a1-49ed-9a6e-5507afcf2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.checkpoint import reshard_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66114c1-4db7-44fd-a2a8-cfd140f9b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = reshard_mp(\n",
    "            state_dicts,\n",
    "            size=1,\n",
    "            rank=0\n",
    "            # moe_num_experts=model_args.moe_args.num_experts,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0abb2083-dd7f-4b7c-aefd-895727439457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['layers.0.feed_forward.global_gate_stats_3E', 'layers.0.feed_forward.running_gate_stats_3E', 'layers.1.feed_forward.running_gate_stats_3E', 'layers.1.feed_forward.global_gate_stats_3E', 'layers.2.feed_forward.running_gate_stats_3E', 'layers.2.feed_forward.global_gate_stats_3E', 'layers.3.feed_forward.running_gate_stats_3E', 'layers.3.feed_forward.global_gate_stats_3E', 'layers.4.feed_forward.global_gate_stats_3E', 'layers.4.feed_forward.running_gate_stats_3E', 'layers.5.feed_forward.running_gate_stats_3E', 'layers.5.feed_forward.global_gate_stats_3E', 'layers.6.feed_forward.global_gate_stats_3E', 'layers.6.feed_forward.running_gate_stats_3E', 'layers.7.feed_forward.running_gate_stats_3E', 'layers.7.feed_forward.global_gate_stats_3E', 'layers.8.feed_forward.global_gate_stats_3E', 'layers.8.feed_forward.running_gate_stats_3E', 'layers.9.feed_forward.running_gate_stats_3E', 'layers.9.feed_forward.global_gate_stats_3E', 'layers.10.feed_forward.global_gate_stats_3E', 'layers.10.feed_forward.running_gate_stats_3E', 'layers.11.feed_forward.global_gate_stats_3E', 'layers.11.feed_forward.running_gate_stats_3E', 'layers.12.feed_forward.running_gate_stats_3E', 'layers.12.feed_forward.global_gate_stats_3E', 'layers.13.feed_forward.global_gate_stats_3E', 'layers.13.feed_forward.running_gate_stats_3E', 'layers.14.feed_forward.global_gate_stats_3E', 'layers.14.feed_forward.running_gate_stats_3E', 'layers.15.feed_forward.global_gate_stats_3E', 'layers.15.feed_forward.running_gate_stats_3E', 'layers.16.feed_forward.running_gate_stats_3E', 'layers.16.feed_forward.global_gate_stats_3E', 'layers.17.feed_forward.global_gate_stats_3E', 'layers.17.feed_forward.running_gate_stats_3E', 'layers.18.feed_forward.running_gate_stats_3E', 'layers.18.feed_forward.global_gate_stats_3E', 'layers.19.feed_forward.running_gate_stats_3E', 'layers.19.feed_forward.global_gate_stats_3E', 'layers.20.feed_forward.running_gate_stats_3E', 'layers.20.feed_forward.global_gate_stats_3E', 'layers.21.feed_forward.global_gate_stats_3E', 'layers.21.feed_forward.running_gate_stats_3E', 'layers.22.feed_forward.global_gate_stats_3E', 'layers.22.feed_forward.running_gate_stats_3E', 'layers.23.feed_forward.global_gate_stats_3E', 'layers.23.feed_forward.running_gate_stats_3E', 'layers.24.feed_forward.running_gate_stats_3E', 'layers.24.feed_forward.global_gate_stats_3E', 'layers.25.feed_forward.global_gate_stats_3E', 'layers.25.feed_forward.running_gate_stats_3E', 'layers.26.feed_forward.running_gate_stats_3E', 'layers.26.feed_forward.global_gate_stats_3E', 'layers.27.feed_forward.running_gate_stats_3E', 'layers.27.feed_forward.global_gate_stats_3E', 'layers.28.feed_forward.global_gate_stats_3E', 'layers.28.feed_forward.running_gate_stats_3E', 'layers.29.feed_forward.global_gate_stats_3E', 'layers.29.feed_forward.running_gate_stats_3E', 'layers.30.feed_forward.running_gate_stats_3E', 'layers.30.feed_forward.global_gate_stats_3E', 'layers.31.feed_forward.global_gate_stats_3E', 'layers.31.feed_forward.running_gate_stats_3E', 'layers.32.feed_forward.running_gate_stats_3E', 'layers.32.feed_forward.global_gate_stats_3E', 'layers.33.feed_forward.global_gate_stats_3E', 'layers.33.feed_forward.running_gate_stats_3E', 'layers.34.feed_forward.running_gate_stats_3E', 'layers.34.feed_forward.global_gate_stats_3E', 'layers.35.feed_forward.running_gate_stats_3E', 'layers.35.feed_forward.global_gate_stats_3E', 'layers.36.feed_forward.running_gate_stats_3E', 'layers.36.feed_forward.global_gate_stats_3E', 'layers.37.feed_forward.global_gate_stats_3E', 'layers.37.feed_forward.running_gate_stats_3E', 'layers.38.feed_forward.running_gate_stats_3E', 'layers.38.feed_forward.global_gate_stats_3E', 'layers.39.feed_forward.global_gate_stats_3E', 'layers.39.feed_forward.running_gate_stats_3E', 'layers.40.feed_forward.global_gate_stats_3E', 'layers.40.feed_forward.running_gate_stats_3E', 'layers.41.feed_forward.running_gate_stats_3E', 'layers.41.feed_forward.global_gate_stats_3E', 'layers.42.feed_forward.running_gate_stats_3E', 'layers.42.feed_forward.global_gate_stats_3E', 'layers.43.feed_forward.running_gate_stats_3E', 'layers.43.feed_forward.global_gate_stats_3E', 'layers.44.feed_forward.global_gate_stats_3E', 'layers.44.feed_forward.running_gate_stats_3E', 'layers.45.feed_forward.running_gate_stats_3E', 'layers.45.feed_forward.global_gate_stats_3E', 'layers.46.feed_forward.running_gate_stats_3E', 'layers.46.feed_forward.global_gate_stats_3E', 'layers.47.feed_forward.running_gate_stats_3E', 'layers.47.feed_forward.global_gate_stats_3E'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(model_args)\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba67476b-978c-4118-a420-b9bcf879b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.datatypes import GenerationResult, QuantizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847313ab-fbd2-47a4-b6c8-7e0eef0b0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_mode = QuantizationMode.int4_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d8371d-e904-4df6-8c84-0c24be776caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.llama4.quantization.loader import convert_to_quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c26091d8-532b-46e9-9be5-d79ae8b5878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.quantize_impls import (\n",
    "    # Fp8ScaledWeights,\n",
    "    Int4ScaledWeights,\n",
    "    # load_fp8,\n",
    "    load_int4,\n",
    "    # quantize_fp8,\n",
    "    quantize_int4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7f5661b-1d73-4fac-b813-c042a96e41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb255924-fac4-45cf-bc5b-95db27fabf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "int4_scales_path = os.path.join(ckpt_dir, f\"int4_scales_{rank}.pt\")\n",
    "\n",
    "\n",
    "def apply_quantization(_, weight):\n",
    "    return quantize_int4(weight, output_device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1f5b67-0085-41e5-8796-ddd385b17a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def should_quantize_block(block: nn.Module) -> bool:\n",
    "        if not isinstance(block, TransformerBlock):\n",
    "            return False\n",
    "\n",
    "        is_moe = isinstance(block.feed_forward, MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "887362ba-d276-432a-b3f7-9e28befe08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, block in model.named_modules():\n",
    "#     if not should_quantize_block(block):\n",
    "#         continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f12c339-3164-46dd-9137-c339e4263960",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for _, block in model.named_modules():\n",
    "            if not should_quantize_block(block):\n",
    "                continue\n",
    "\n",
    "            update_status(f\"Rank {rank} - Layer {block.layer_id}\")\n",
    "\n",
    "            # Quantize only routed experts, not shared\n",
    "            prefix = f\"layers.{block.layer_id}.feed_forward\"\n",
    "            moe = block.feed_forward\n",
    "            moe.experts.batched_swiglu = experts_batched_swiglu_wrapper.__get__(moe.experts)\n",
    "\n",
    "            for key in (\"w1\", \"w3\", \"w2\"):\n",
    "                param = getattr(moe.experts, key)\n",
    "                update_status(f\"Rank {rank} - Layer {block.layer_id} - MoE {key}\")\n",
    "                setattr(\n",
    "                    moe.experts,\n",
    "                    key,\n",
    "                    apply_quantization(\n",
    "                        f\"{prefix}.experts.{key}\",\n",
    "                        param.transpose(1, 2).contiguous(),\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            if True: #quantization_mode == QuantizationMode.int4_mixed:\n",
    "                # Quantize shared experts\n",
    "                moe.shared_expert.forward = swiglu_wrapper_no_reduce.__get__(moe.shared_expert)\n",
    "                for key in (\"w1\", \"w3\", \"w2\"):\n",
    "                    param = getattr(moe.shared_expert, key)\n",
    "                    update_status(f\"Rank {rank} - Layer {block.layer_id} - MoE shared expert {key}\")\n",
    "                    param.weight = apply_quantization(f\"{prefix}.shared_expert.{key}\", param.weight)\n",
    "\n",
    "            processed_blocks += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d492341-ca4b-4790-bf87-e516929c6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7939eac40ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           2.0Ti       656Gi       585Gi       7.1Gi       787Gi       1.3Ti\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131abab-c034-4737-9514-e552aa56e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842591cb-39ea-469c-bf82-6baf9b03feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03128b0-b02b-4914-b593-4595dda6eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_quantize_block(block: nn.Module) -> bool:\n",
    "    if not isinstance(block, TransformerBlock):\n",
    "        return False\n",
    "\n",
    "    is_moe = isinstance(block.feed_forward, MoE)\n",
    "    if quantization_mode == QuantizationMode.fp8_mixed:\n",
    "        # skip quantization on first and last layers\n",
    "        return is_moe and not (block.layer_id == 0 or block.layer_id == (model.n_layers - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce8a6d-a8ec-46ee-b052-c7f653ba715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convert_to_quantized_model(model, ckpt_dir, quantization_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63580b48-1cf2-4c98-b1b3-9496bd8541c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fbgemm-gpu --index-url https://download.pytorch.org/whl/cu126/\n",
    "# !pip install --pre fbgemm-gpu --index-url https://download.pytorch.org/whl/nightly/cu126/\n",
    "# !pip install fbgemm-gpu==1.2.0 --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install fbgemm-gpu-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345dd0f-4fca-4a85-be11-847295cd4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y fbgemm-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b0c2d-80e1-4b00-b723-f30b4056bd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934f947-36f0-40c5-823d-93f544faa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112aaffa-a611-4536-ac1a-7766e287db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f90a4-afee-4e66-81cf-f013f95183b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"How are you?\"\n",
    "\n",
    "tokens = torch.Tensor(tokenizer.encode(x, bos=False, eos=False)).unsqueeze(dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43ec0c-5a04-4e04-b52c-d7ab8ace7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e4504-cebd-4a35-83c2-bd41292b77ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
